# -*- coding: utf-8 -*-
"""VM0047_v7_multitemporal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k19FgKhOEu4PqNXhfGlqDGgW-meVq2rf

üåé Cell 1: Setup - Imports & Authentication
This first cell imports all necessary Python libraries and handles the one-time authentication and initialization for Google Earth Engine (GEE).

Note: When you run ee.Authenticate(), you will need to follow a link to authorize your Google account and paste the resulting token back into the input box.
"""

import ee
import geemap
import geopandas as gpd
from datetime import datetime
import math
import matplotlib.pyplot as plt
import numpy as np
import os # Import os for path joining
from google import genai
from google.colab import userdata
from textwrap import dedent
import io
import sys

# Authenticate and initialize Earth Engine (run only once)
ee.Authenticate() # Uncomment only for the first run
ee.Initialize(project='geometric-wall-406011', opt_url='https://earthengine-highvolume.googleapis.com')

# Initialize geemap
Map = geemap.Map()
Map.set_options('SATELLITE')

try:
    import geemap
    m = geemap.Map()
    print("Geemap library loaded for interactive mapping.")
except ImportError:
    m = None
    print("Geemap library not found. Mapping functionality will be skipped.")
    print("To enable mapping, install geemap: pip install geemap")

"""‚öôÔ∏è Cell 2: Section 1 - User Configuration
This cell contains all the main configuration parameters for the analysis. You can modify the values in the CONFIG dictionary, the time_periods list, and the studyAreaAsset path to suit your own project.
"""

#=======================
# Section 1: User Configuration
# ==========
CONFIG = {
    'studyAreaAsset': "projects/geometric-wall-406011/assets/forest2",
    'hansenThreshold': 30,
    'sampleSize': 400,
    'trainFraction': 0.7,
    'numTrees': 100,
    'seed': 42,
    'features': [
        'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12',
        'EVI', 'NDFI', 'NDVI', 'SAVI', 'wetness', 'greenness', 'brightness',
        'elevation', 'slope', 'aspect'
    ]
} # Note: Mismatched bracket in source, corrected here. [44] is } and [45] is ].

# Define the three time periods
time_periods = [
    {'startDate': '2019-01-01', 'endDate': '2019-12-28'}, # Corrected syntax from source
    {'startDate': '2022-01-01', 'endDate': '2022-12-28'}, # Corrected syntax from source
    {'startDate': '2025-01-01', 'endDate': '2025-09-28'}  # Corrected syntax from source
]

# Load study area
studyArea = ee.FeatureCollection(CONFIG['studyAreaAsset']).geometry()
if m:
    m.set_options('SATELLITE')
    m.addLayer(studyArea, {'color': 'FF0000', 'opacity': 0.5}, 'Study Area Boundary', False)

"""üõ∞Ô∏è Cell 3: Section 3 - Sensor-Specific Pre-Processing Functions
This cell defines the helper functions used to prepare satellite imagery from different sensors (Landsat 7, Landsat 8, and Sentinel-2). These functions handle tasks like cloud masking and band scaling.
"""

# ==================================================
# Section 3: Sensor-Specific Pre-Processing Functions
# ==================================================
def preprocess_17(image):
    qa = image.select('QA_PIXEL')
    mask = qa.bitwiseAnd(1 << 0).eq(0).And(qa.bitwiseAnd(1 << 1).eq(0)).And(
        qa.bitwiseAnd(1 << 3).eq(0)).And(qa.bitwiseAnd(1 << 4).eq(0)).And(qa.bitwiseAnd(1 << 5).eq(0))
    opticalBands = image.select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5',
        'SR_B7']).multiply(0.0000275).add(-0.2)
    return opticalBands.updateMask(mask)

def preprocess_18(image):
    qa = image.select('QA_PIXEL')
    mask = qa.bitwiseAnd(1 << 3).eq(0).And(qa.bitwiseAnd(1 << 4).eq(0)).And(qa.bitwiseAnd(1 << 5).eq(0))
    opticalBands = image.select(['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6',
        'SR_B7']).multiply(0.0000275).add(-0.2)
    return opticalBands.updateMask(mask)

def preprocess_s2(image):
    scl = image.select('SCL')
    mask = scl.eq(4).Or(scl.eq(5)).Or(scl.eq(6)).Or(scl.eq(11))
    return image.divide(10000).updateMask(
        mask)

"""‚ú® Cell 4: Section 4 - Unified Feature Engineering Function
This cell defines the main function for feature engineering. It takes a satellite image composite and calculates a wide range of spectral indices (like NDVI, EVI) and topographical features (elevation, slope, aspect) that will be used by the machine learning model.
"""

# ==========
# Section 4: Unified Feature Engineering Function
# =============================================
def calculate_all_features(composite, config):
    image = composite.select(config['bands']['common'])
    opticalProjection = image.select('B2').projection()

    ndvi = image.normalizedDifference([config['bands']['nir'], config['bands']['red']]).rename('NDVI')
    evi = image.expression(
        '(2.5 * (NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {
        'NIR': image.select(config['bands']['nir']), 'RED': image.select(config['bands']['red']),
        'BLUE': image.select(config['bands']['blue'])
    }).rename('EVI')
    savi = image.expression(
        '(1.5 * (NIR - RED) / (NIR + RED + 0.5))', {
        'NIR': image.select(config['bands']['nir']), 'RED': image.select(config['bands']['red'])
    }).rename('SAVI')

    smaBands = image.select(config['bands']['sma'])
    sma = smaBands.unmix(config['endmembers']).max(0).rename(['GV', 'NPV', 'Soil', 'Shade'])
    sumFractions = sma.select(['GV', 'NPV', 'Soil']).reduce(ee.Reducer.sum())
    shade = sumFractions.subtract(1.0).abs().rename('Shade')
    gvs = sma.select('GV').divide(shade).rename('GVs')
    ndfi = sma.expression(
        '(GVs - (NPV + Soil)) / (GVs + NPV + Soil)', {
        'GVs': gvs, 'NPV': sma.select('NPV'), 'Soil': sma.select('Soil')
    }).rename('NDFI')

    tctImage = image.select(config['tct']['bands'])
    brightness = tctImage.multiply(config['tct']['brightnessCoeffs']).reduce(ee.Reducer.sum()).rename('brightness')
    greenness = tctImage.multiply(config['tct']['greennessCoeffs']).reduce(ee.Reducer.sum()).rename('greenness')
    wetness = tctImage.multiply(config['tct']['wetnessCoeffs']).reduce(ee.Reducer.sum()).rename('wetness')

    dem = ee.Image('USGS/SRTMGL1_003').clip(studyArea)
    terrain = ee.Terrain.products(dem)
    terrainReprojected = terrain.reproject(
        crs=opticalProjection.crs(),
        scale=opticalProjection.nominalScale()
    ) # Note: Source [111] is just ')'

    return image \
        .addBands([ndvi, evi, savi, ndfi, brightness, greenness, wetness]) \
        .addBands(terrainReprojected.select(['elevation', 'slope', 'aspect']))

"""üìä Cell 5: Section 5 - Reporting & Plotting Functions
This cell defines functions for displaying the results.

print_report formats and prints the classification accuracy and area statistics to the console.

plot_change_barchart is a helper function used later to create bar charts of forest change.
"""

# ===============================================
# Section 5: Reporting and Visualization Functions
# ===============================================
def print_report(prefix, results):
    print('==================================================')
    print(f' {prefix} CLASSIFICATION REPORT')
    print('==================================================')
    print('--- Area Calculations (Hectares) ---')
    print('This section reports areas based on pixel counts and geometry.')
    print(f" - Geometrical Area of Study Region (ha): {results['geometricalAreaHa']:.2f}")
    print(f" - Total Classified Area (ha): {results['totalClassifiedAreaHa']:.2f}")
    print(' Reason: Sum of forest and non-forest areas from raw classification.')
    print(f" - Forest Area (ha): {results['forestArea']:.2f}")
    print(f" - Non-Forest Area (ha): {results['nonForestArea']:.2f}")
    print(f" - Invalid/NoData Area (ha): {results['invalidPixelArea']:.2f}")
    print(' Reason: Areas without valid data due to masking.')
    print('\n--- Post-Processing Areas (Hectares) ---')
    print('This section shows areas after smoothing and filtering for minimum mapping unit.')
    print(f" - Smoothed Forest Area (ha): {results['smoothedForestHa']:.2f}")
    print(' Reason: Focal mode smoothing reduces noise.')
    print(f" - Filtered Forest Area (ha): {results['filteredForestHa']:.2f}")
    print(' Reason: Removes small patches below MMU threshold.')
    print('\n--- Classification Metrics ---')
    print('These metrics evaluate model performance on the test set.')
    print(f" - Overall Accuracy: {results['overallAccuracy']:.4f}")
    print(' Reason: Proportion of correctly classified samples.')
    print(f" - Producer's Accuracy (Forest/Non-Forest): {results['producersAccuracy']}")
    print(' Reason: Accuracy from the perspective of actual classes.')
    print(f" - User's Accuracy (Forest/Non-Forest): {results['consumersAccuracy']}")
    print(' Reason: Accuracy from the perspective of predicted classes.')
    print(f" - Confusion Matrix (Actual Rows vs Predicted Columns): {results['errorMatrix']}")
    print('\n--- Data & Configuration ---')
    print(' Summary of input data and processing parameters.')
    print(f" - Total Images in Processed Collection: {results['imageCount']}")
    print(' Reason: Number of images after filtering and preprocessing.')
    print(f" - Training Set Size (points): {results['trainingSetSize']}")
    print(f" - Testing Set Size (points): {results['testingSetSize']}")
    print(' Reason: Split based on trainFraction for validation.')

# Helper function for plotting change barchart
def plot_change_barchart(change_stats, title, ax):
    """Plots a barchart of change statistics."""
    labels = list(change_stats.keys())
    values = [change_stats[label] for label in labels]
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'] # Consistent colors
    ax.bar(labels, values, color=colors)
    ax.set_title(title, fontsize=14)
    ax.set_ylabel('Area (Hectares)', fontsize=12)
    ax.tick_params(axis='x', rotation=45, labelsize=10)
    ax.grid(axis='y', linestyle='--', alpha=0.7) # Merged lines
    # Add text labels on bars
    for i, v in enumerate(values):
        ax.text(i, v + 0.1, f"{v:.2f}", ha='center', va='bottom', fontsize=9) # Source has 'ha' and '¬∞' typo

"""üöÄ Cell 6: Sections 6 & 7 - Main Analysis & Export Loop
This is the main computational cell. It loops over each time_period defined in Cell 2 and performs the complete analysis workflow:

Selects the correct sensor (Landsat 7, 8, or Sentinel-2).

Acquires and preprocesses the imagery.

Calculates all features using the function from Cell 4.

Prepares training labels using Hansen Forest data and (if available) Dynamic World.

Trains a Random Forest classifier and classifies the image.

Post-processes the classification (smoothing and filtering).

Calculates final areas.

Prints the report for the period using the function from Cell 5.

Stores the results in memory for the change analysis in the next cell.

Exports all results (images and training points) to your Google Drive.

Warning: This cell will take a significant amount of time to run, as it is executing the entire analysis for all time periods and queuing export tasks.
"""

# =====================================================================
# Section 6: Main Analysis Workflow (Modified for Multiple Time Periods)
# =====================================================================
# NEW ADDITION: Create a dictionary to store results
analysis_results = {}
# ADD THESE LINES HERE:
client_results_all_years = {}
sensor_configs_by_year = {}

#--- Start of the new robust loop
for period in time_periods:
    # Wrap the entire period's analysis in a try...except block
    try:
        print(f"\n===== Processing Time Period: {period['startDate']} to {period['endDate']} =====")
        # Update CONFIG with current period's dates
        CONFIG['startDate'] = period['startDate']
        CONFIG['endDate'] = period['endDate']

        #Subsection A: Sensor Selection and Configuration
        sensorConfig = {}
        startDate = CONFIG['startDate']
        l7Cutoff = '2013-03-18'
        s2Cutoff = '2017-03-07'

        if startDate < l7Cutoff:
            sensorConfig = {
                'name': 'Landsat 7',
                'collection': 'LANDSAT/LE07/C02/T1_L2',
                'scale': 30,
                'preprocess': preprocess_17,
                'bands': {
                    'optical': ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7'],
                    'common': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7'],
                    'blue': 'B1', 'green': 'B2', 'red': 'B3', 'nir': 'B4', 'swir1': 'B5', 'swir2': 'B7',
                    'sma': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7']
                },
                'endmembers': [[0.01, 0.03, 0.02, 0.35, 0.23, 0.10], [0.04, 0.08, 0.12, 0.12, 0.30, 0.12],
                    [0.12, 0.15, 0.17, 0.20, 0.35, 0.30], [0, 0, 0, 0, 0, 0]],
                'tct': {
                    'bands': ee.List(['B1', 'B2', 'B3', 'B4', 'B5', 'B7']),
                    'brightnessCoeffs': ee.Image.constant([0.3561, 0.3972, 0.3904, 0.6966, 0.2286, 0.1596]),
                    'greennessCoeffs': ee.Image.constant([-0.3344, -0.3544, -0.4556, 0.6966, -0.0242, -0.2630]),
                    'wetnessCoeffs': ee.Image.constant([0.2626, 0.2141, 0.0926, 0.0656, -0.7629, -0.5388])
                }
            }
        elif startDate < s2Cutoff:
            sensorConfig = {
                'name': 'Landsat 8',
                'collection': 'LANDSAT/LC08/C02/T1_L2',
                'scale': 30,
                'preprocess': preprocess_18,
                'bands': {
                    'optical': ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7'],
                    'common': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7'],
                    'blue': 'B2', 'green': 'B3', 'red': 'B4', 'nir': 'B5', 'swir1': 'B6', 'swir2': 'B7',
                    'sma': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']
                },
                'endmembers': [[0.01, 0.03, 0.02, 0.35, 0.23, 0.10], [0.04, 0.08, 0.12, 0.12, 0.30, 0.12], [0.12, 0.15, 0.17, 0.20, 0.35, 0.30], [0, 0, 0, 0, 0, 0]],
                'tct': {
                    'bands': ee.List(['B2', 'B3', 'B4', 'B5', 'B6', 'B7']),
                    'brightnessCoeffs': ee.Image.constant([0.3029, 0.2786, 0.4733, 0.5599, 0.5080, 0.1872]),
                    'greennessCoeffs': ee.Image.constant([-0.2941, -0.2430, -0.5424, 0.7276, 0.0713, -0.1608]),
                    'wetnessCoeffs': ee.Image.constant([0.1511, 0.1973, 0.3283, 0.3407, -0.7117, -0.4559])
                }
            }
        else:
            sensorConfig = {
                'name': 'Sentinel-2',
                'collection': 'COPERNICUS/S2_SR_HARMONIZED',
                'scale': 10,
                'preprocess': preprocess_s2,
                'bands': {
                    'optical': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'],
                    'common': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'],
                    'blue': 'B2', 'green': 'B3', 'red': 'B4', 'nir': 'B8', 'swir1': 'B11', 'swir2': 'B12',
                    'sma': ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
                },
                'endmembers': [[0.0119, 0.0475, 0.0169, 0.625, 0.2399, 0.0675],
                    [0.1514, 0.1597, 0.1421, 0.3053, 0.7707, 0.1975],
                    [0.1799, 0.2479, 0.3158, 0.5437, 0.7707, 0.6646], [0, 0, 0, 0, 0, 0]],
                'tct': {
                    'bands': ee.List(['B2', 'B3', 'B4', 'B8', 'B11', 'B12']),
                    'brightnessCoeffs': ee.Image.constant([0.3037, 0.2793, 0.4743, 0.5585, 0.5082, 0.1863]),
                    'greennessCoeffs': ee.Image.constant([-0.2848, -0.2435, -0.5436, 0.7243, 0.0840, -0.1800]),
                    'wetnessCoeffs': ee.Image.constant([0.1509, 0.1973, 0.3279, 0.3406, -0.7112, -0.4572])
                }
            }

        print('Sensor Selection:')
        print(f" Selected {sensorConfig['name']} based on start date ({startDate}).")
        print('Reason: Ensures compatibility with available data and optimal resolution.')

        #Subsection B: Data Acquisition and Composite Creation
        imageCollection = ee.ImageCollection(sensorConfig['collection']) \
            .filterBounds(studyArea) \
            .filterDate(CONFIG['startDate'], CONFIG['endDate'])

        image_count = imageCollection.size().getInfo()
        print('Data Acquisition Debug:')
        print(f" Initial Image Count in Collection (before preprocessing): {image_count}")
        print('Reason: Verifies sufficient data availability in the date range.')

        #--- CRITICAL CHECK---
        # If no images, skip this period and continue the loop
        if image_count == 0:
            print(f" *** ERROR: No images found for {startDate}. Skipping this period. ***")
            continue # This jumps to the next 'period' in the main 'for' loop

        def process_and_rename(image):
            processed = sensorConfig['preprocess'](image)
            return processed.select(sensorConfig['bands']['optical']).rename(sensorConfig['bands']['common'])

        processedCollection = imageCollection.map(process_and_rename)

        #Check if the collection is *still* empty after preprocessing (e.g., all images were cloudy)
        first_processed = processedCollection.first()
        if first_processed is None:
            print(f" *** ERROR: All images were masked out (e.g., cloud). Skipping this period. ***")
            continue

        targetCrs = first_processed.select('B2').projection().crs()
        composite = processedCollection.median() \
            .reproject(crs=targetCrs, scale=sensorConfig['scale'])

        if m:
            visParams = {'min': 0, 'max': [0.3, 0.3, 0.3], 'bands': ['B4', 'B3', 'B2'], 'opacity': 0.5}
            m.addLayer(composite.clip(studyArea), visParams, f'1. RGB Composite {startDate[:4]}', False)

        # Subsection C: Feature Engineering and Filtering
        fullFeatureStack = calculate_all_features(composite, sensorConfig)
        availableFeatures = fullFeatureStack.bandNames()
        finalFeatures = ee.List(CONFIG['features']).filter(ee.Filter.inList('item', availableFeatures))
        classificationStack = fullFeatureStack.select(finalFeatures)

        print('Feature Engineering:')
        print(f" Features Used for Classification (Filtered by Sensor Availability): {finalFeatures.getInfo()}")
        print(' Reason: Ensures only compatible bands/indices are used, avoiding errors.')

        # Subsection D: Label Preparation
        hansen = ee.Image('UMD/hansen/global_forest_change_2024_v1_12')
        endYear = ee.Number.parse(ee.String(CONFIG['endDate']).slice(0, 4))
        max_loss_year = endYear.subtract(2000).min(24)
        GFC_Forest = hansen.select("treecover2000").gte(CONFIG['hansenThreshold'])
        hansenLoss = hansen.select('lossyear').unmask(0)

        lostBeforeOrDuringTarget = hansenLoss.gt(0).And(hansenLoss.lte(max_loss_year))
        GFC_Forest_Unlost = GFC_Forest.And(lostBeforeOrDuringTarget.eq(0))
        GFC_NonForest = hansen.select("treecover2000").lt(CONFIG['hansenThreshold'])

        DW_START_DATE = '2017-06-27'
        start_date_obj = datetime.strptime(CONFIG['startDate'], '%Y-%m-%d')
        dw_start_date_obj = datetime.strptime(DW_START_DATE, '%Y-%m-%d')
        DW_Available = start_date_obj >= dw_start_date_obj

        if DW_Available:
            print('Label Preparation:')
            print(' Dynamic World Available for Date Range. Using Hansen + DW Intersection.')
            print(' Reason: Improves label accuracy by combining tree cover with land cover classes.')
            DW_MODE = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1') \
                .filterBounds(studyArea) \
                .filterDate(CONFIG['startDate'], CONFIG['endDate']) \
                .select('label') \
                .mode()
            DW_Forest = DW_MODE.eq(1)
            Forest_Mask = GFC_Forest_Unlost.updateMask(DW_Forest).unmask(0).eq(1)
            NonForest_Mask = GFC_NonForest.updateMask(DW_MODE.neq(1)).unmask(0).eq(1)
        else:
            print('Label Preparation:')
            print(f' Dynamic World NOT Available (< {DW_START_DATE}). Using Hansen-Only Time Adjusted Mask.')
            print(' Reason: Falls back to Hansen for older dates, adjusting for loss years.')
            Forest_Mask = GFC_Forest_Unlost.unmask(0).eq(1)
            GFC_NonForest_Temporal = GFC_NonForest.Or(lostBeforeOrDuringTarget)
            NonForest_Mask = GFC_NonForest_Temporal.unmask(0).eq(1)

        labelImage = ee.Image(0).rename('label') \
            .where(Forest_Mask.eq(1), 1) \
            .updateMask(Forest_Mask.eq(1).Or(NonForest_Mask.eq(1)))

        pixelCountForest = Forest_Mask.rename('mask').reduceRegion(
            reducer=ee.Reducer.sum(), geometry=studyArea, scale=sensorConfig['scale'], maxPixels=1e13
        ).get('mask')
        pixelCountNonForest = NonForest_Mask.rename('mask').reduceRegion(
            reducer=ee.Reducer.sum(), geometry=studyArea, scale=sensorConfig['scale'], maxPixels=1e13
        ).get('mask')

        print('Label Debug:')
        print(' Available Pixels for Sampling (Forest/Non-Forest):')
        print(f" Forest: {pixelCountForest.getInfo()} / Non-Forest: {pixelCountNonForest.getInfo()}")
        print(' Reason: Ensures balanced classes for stratified sampling.')

        # --- CRITICAL CHECK ---
        # Check if either class has 0 pixels.
        if ee.Number(pixelCountForest).getInfo() == 0 or ee.Number(pixelCountNonForest).getInfo() == 0:
            print(f" *** ERROR: No valid training pixels found for one or both classes. Skipping this period. ***")
            continue

        # Subsection E: Classification and Accuracy Assessment
        trainingData = classificationStack.addBands(labelImage).stratifiedSample(
            numPoints=CONFIG['sampleSize'], classBand='label', region=studyArea,
            scale=sensorConfig['scale'], seed=CONFIG['seed'], geometries=True
        )
        withRandom = trainingData.randomColumn('random', CONFIG['seed'])
        trainingSet = withRandom.filter(ee.Filter.lt('random', CONFIG['trainFraction']))
        testingSet = withRandom.filter(ee.Filter.gte('random', CONFIG['trainFraction']))

        print('Classification Debug:')
        print(f" Sampled Points Count (Total/Train/Test): {trainingData.size().getInfo()}/{trainingSet.size().getInfo()}/{testingSet.size().getInfo()}")
        print(' Reason: Confirms split ratio and sufficient samples for training.')

        forestPoints = trainingData.filter(ee.Filter.eq('label', 1))
        nonForestPoints = trainingData.filter(ee.Filter.eq('label', 0))

        if m:
            m.addLayer(forestPoints, {'color': '00FF00'}, f'7. Training Data Points (Forest) {startDate[:4]}', False)
            m.addLayer(nonForestPoints, {'color': 'FF0000'}, f'7.1 Training Data Points (Non-Forest) {startDate[:4]}', False)

        classifier = ee.Classifier.smileRandomForest(
            numberOfTrees=CONFIG['numTrees'],
            seed=CONFIG['seed']
        ).train(
            features=trainingSet,
            classProperty='label',
            inputProperties=classificationStack.bandNames()
        )

        classified = classificationStack.classify(classifier).rename('classification')
        validated = testingSet.classify(classifier)
        errorMatrix = validated.errorMatrix('label', 'classification')
        overallAccuracy = errorMatrix.accuracy()
        producersAccuracy = errorMatrix.producersAccuracy()
        consumersAccuracy = errorMatrix.consumersAccuracy()
        errorMatrixArray = errorMatrix.array()

        if m:
            classVis = {'min': 0, 'max': 1, 'palette': ['yellow', 'green'], 'opacity': 0.5}
            m.addLayer(classified.clip(studyArea), classVis, f'8. Classification (Raw) {startDate[:4]}', False)

        # Subsection F: Post-Processing (Smoothing & Filtering)
        scaleSq = sensorConfig['scale'] * sensorConfig['scale']
        MMU_HA = 1
        MMU_M2 = MMU_HA * 10000
        minPixelCount = math.ceil(MMU_M2 / scaleSq)

        print('Post-Processing Config:')
        print(' Dynamic Minimum Mapping Unit (MMU):')
        print(f" - Sensor Scale (m): {sensorConfig['scale']}")
        print(f" - Required Pixels for 1 ha: {minPixelCount}")
        print(' Reason: Ensures patches meet size threshold based on sensor resolution.')

        smoothed = classified.focal_mode(radius=2, kernelType='square', units='pixels').rename('classification')

        if m:
            m.addLayer(smoothed.clip(studyArea), classVis, f'9. Classification (Smoothed) {startDate[:4]}', False)

        forestPatches = smoothed.eq(1)
        forestPatchSize = forestPatches.connectedPixelCount(100, True)
        forestFiltered = forestPatches.updateMask(forestPatchSize.gte(minPixelCount))
        cleaned = forestFiltered.unmask(0).rename('classification')

        # ADD THIS LINE HERE TO ADD THE CLEANED LAYER TO THE MAP FOR THIS YEAR
        if m:
            classVis = {'min': 0, 'max': 1, 'palette': ['yellow', 'green'], 'opacity': 0.7} # Increased opacity for final layer
            m.addLayer(cleaned.clip(studyArea), classVis, f'10. Classification (Filtered) {startDate[:4]}', True) # Set visible by default

        # Subsection G: Area Calculations
        pixelAreaHa = ee.Image.pixelArea().divide(10000)
        reducer = ee.Reducer.sum()
        scale = sensorConfig['scale']

        def calc_area(image):
            return image.multiply(pixelAreaHa).reduceRegion(reducer=reducer, geometry=studyArea, scale=scale, maxPixels=1e13)

        forestArea = calc_area(classified.eq(1)).get('classification')
        nonForestArea = calc_area(classified.eq(0)).get('classification')

        dataMask = composite.select(0).mask()
        invalidPixelMask = dataMask.Not().rename('invalid')

        if m:
            m.addLayer(invalidPixelMask.updateMask(invalidPixelMask.gt(0)), {'palette': ['FF00FF'], 'opacity': 0.5}, f'11. Invalid/NoData Pixels {startDate[:4]}', False)

        invalidPixelArea = calc_area(invalidPixelMask).get('invalid')
        smoothedForestHa = calc_area(smoothed.eq(1)).get('classification')
        smoothedNonForestHa = calc_area(smoothed.eq(0)).get('classification')
        filteredForestHa = calc_area(cleaned.eq(1)).get('classification')
        filteredNonForestHa = calc_area(cleaned.eq(0)).get('classification')

        # Subsection H: Finalize and Display Results
        results = {
            'geometricalAreaHa': studyArea.area().divide(10000),
            'forestArea': forestArea,
            'nonForestArea': nonForestArea,
            'totalClassifiedAreaHa': ee.Number(forestArea).add(ee.Number(nonForestArea)),
            'invalidPixelArea': invalidPixelArea,
            'errorMatrix': errorMatrixArray,
            'overallAccuracy': overallAccuracy,
            'producersAccuracy': producersAccuracy,
            'consumersAccuracy': consumersAccuracy,
            'smoothedForestHa': smoothedForestHa,
            'smoothedNonForestHa': smoothedNonForestHa,
            'filteredForestHa': filteredForestHa,
            'filteredNonForestHa': filteredNonForestHa,
            'imageCount': processedCollection.size(),
            'trainingSetSize': trainingSet.size(),
            'testingSetSize': testingSet.size()
        }

        client_results = ee.Dictionary(results).getInfo()

        # --- NEW ADDITION: Store results for change analysis ---
        year = period['startDate'][:4]
        analysis_results[year] = {
            'image': cleaned,
            'projection': classificationStack.projection(),
            'scale': sensorConfig['scale'],
            'filtered_forest_ha': client_results['filteredForestHa']
        }
        # --- END OF ADDITION ---

        print_report(f"{sensorConfig['name']} ({startDate[:4]})", client_results)

        # ADD THESE LINES HERE:
        year = period['startDate'][:4]
        client_results_all_years[year] = client_results
        sensor_configs_by_year[year] = sensorConfig

        # Subsection I: Pixel Size Verification
        bandToCheck = classificationStack.select('B2')
        topoBand = classificationStack.select('slope')
        pixelSizeOptical = bandToCheck.projection().nominalScale()
        pixelSizeTopo = topoBand.projection().nominalScale()

        print('Projection Verification:')
        print(' Final Pixel Scale Check (Must be Equal and Match Sensor Scale):')
        print(f" - Optical Band (B2) Pixel Size: {pixelSizeOptical.getInfo()}")
        print(f" - Topographic Band (Slope) Pixel Size: {pixelSizeTopo.getInfo()}")
        print(' Reason: Confirms reprojection success; mismatches cause processing errors.')

        # ==========================================================================================
        # Section 7: Exporting Results to Google Drive
        # ==========================================================================================

        print(f"\n--- Starting Exports for {startDate[:4]} ---")
        exportFolder = 'GEE_Forest_Export'
        year = period['startDate'][:4]
        sensorName = sensorConfig['name'].replace(' ', '_')
        baseName = f"{sensorName}_{year}"
        exportScale = sensorConfig['scale']
        exportCrs = classificationStack.projection().crs()

        task_points = ee.batch.Export.table.toDrive(
            collection=trainingData,
            description=f"{baseName}_Labels_Points",
            folder=exportFolder,
            fileNamePrefix=f"{baseName}_Labels_Points",
            fileFormat='SHP'
        )
        task_points.start()
        print(f" ... Task started: {baseName}_Labels_Points")

        task_label_img = ee.batch.Export.image.toDrive(
            image=labelImage.rename('label').toByte(),
            description=f"{baseName}_Labels_Image",
            folder=exportFolder,
            fileNamePrefix=f"{baseName}_Labels_Image",
            region=studyArea,
            scale=exportScale,
            crs=exportCrs,
            maxPixels=1e13
        )
        task_label_img.start()
        print(f" ... Task started: {baseName}_Labels_Image")

        task_class_raw = ee.batch.Export.image.toDrive(
            image=classified.toByte(),
            description=f"{baseName}_Classified_Raw",
            folder=exportFolder,
            fileNamePrefix=f"{baseName}_Classified_Raw",
            region=studyArea,
            scale=exportScale,
            crs=exportCrs,
            maxPixels=1e13
        )
        task_class_raw.start()
        print(f" ... Task started: {baseName}_Classified_Raw")

        task_class_smooth = ee.batch.Export.image.toDrive(
            image=smoothed.toByte(),
            description=f"{baseName}_Classified_Smoothed",
            folder=exportFolder,
            fileNamePrefix=f"{baseName}_Classified_Smoothed",
            region=studyArea,
            scale=exportScale,
            crs=exportCrs,
            maxPixels=1e13
        )
        task_class_smooth.start()
        print(f" ... Task started: {baseName}_Classified_Smoothed")

        task_class_cleaned = ee.batch.Export.image.toDrive(
            image=cleaned.toByte(),
            description=f"{baseName}_Classified_Cleaned",
            folder=exportFolder,
            fileNamePrefix=f"{baseName}_Classified_Cleaned",
            region=studyArea,
            scale=exportScale,
            crs=exportCrs,
            maxPixels=1e13
        )
        task_class_cleaned.start()
        print(f" ... Task started: {baseName}_Classified_Cleaned")

        task_feature_stack = ee.batch.Export.image.toDrive(
            image=classificationStack.toFloat(),
            description=f"{baseName}_Feature_Stack",
            folder=exportFolder,
            fileNamePrefix=f"{baseName}_Feature_Stack",
            region=studyArea,
            scale=exportScale,
            crs=exportCrs,
            maxPixels=1e13
        )
        task_feature_stack.start()
        print(f" ... Task started: {baseName}_Feature_Stack")

    # This 'except' block catches any error from the 'try' block above
    except Exception as e:
        print(f"\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        print(f" ERROR processing period {period['startDate']}: {e}")
        print(f" This period will be SKIPPED.")
        print(f"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")
        # 'pass' tells the loop to do nothing and just move to the next 'period'
        pass
# --- End of the new robust loop ---

"""üîÑ Cell 7: Section 8 - Change Detection & Visualization
This cell runs after the main loop in Cell 6 is complete. It uses the analysis_results dictionary (which was populated during the loop) to perform a change detection analysis. It calculates the change between the first and last successful time periods and generates plots showing:

Total forest area over time.

Detailed change dynamics (e.g., deforestation, reforestation) between consecutive periods.
"""

# ===================================================================
# Section 8: Change Detection and Visualization
# ===================================================================
# This new section runs after the loop above is complete.
# It uses the 'analysis_results' dictionary populated during the loop.
# ===================================================================

print("\n====================================================")
print(" STARTING POST-LOOP CHANGE ANALYSIS")
print("====================================================")

# Helper function to calculate change areas
def compute_change_stats(img_t1, img_t2, studyArea, scale):
    """Calculates a change map and computes areas for each change class."""
    # Create a binary change map
    # 0 = Stable Non-Forest (0 -> 0)
    # 1 = Reforestation (0 -> 1)
    # 2 = Deforestation (1 -> 0)
    # 3 = Stable Forest (1 -> 1)
    change_image = (img_t1.multiply(2)).add(img_t2).rename('classification')

    pixelAreaHa = ee.Image.pixelArea().divide(10000)
    reducer = ee.Reducer.sum()

    # Re-using the area calculation logic from the main script
    def calc_area(image):
        return image.multiply(pixelAreaHa).reduceRegion(
            reducer=reducer,
            geometry=studyArea,
            scale=scale,
            maxPixels=1e13
        ).get('classification')

    # Calculate area for each class
    stats = {
        'Stable Non-Forest': calc_area(change_image.eq(0)),
        'Reforestation': calc_area(change_image.eq(1)),
        'Deforestation': calc_area(change_image.eq(2)),
        'Stable Forest': calc_area(change_image.eq(3))
    }
    # Return the computed dictionary
    return ee.Dictionary(stats).getInfo()

# Get the years from the analysis_results dictionary
years = sorted(analysis_results.keys())

# Check if enough periods were successfully processed
if len(years) >= 2:
    print(f"Found {len(years)} successfully processed time periods: {years}. Proceeding with analysis.")

    # --- A. Data Preparation ---
    # Get the images for the first and last successful periods
    img_t1 = analysis_results[years[0]]['image']
    img_t2 = analysis_results[years[-1]]['image']

    # Get the projection and scale from the first successful period to use as a common grid
    target_proj = analysis_results[years[0]]['projection']
    target_scale = analysis_results[years[0]]['scale']

    print(f"Resampling classifications to a common {target_scale}m grid for comparison.")

    # Resample the images to the target projection
    img_t1_res = img_t1.reproject(crs=target_proj.crs(), scale=target_scale)
    img_t2_res = img_t2.reproject(crs=target_proj.crs(), scale=target_scale)

    # --- B. Run Change Analysis ---
    # Calculate change between the first and last periods
    print(f"\nCalculating Change: {years[0]} vs {years[-1]}...")
    change_t1_t2 = compute_change_stats(img_t1_res, img_t2_res, studyArea, target_scale)
    print(change_t1_t2)

    # Calculate change for consecutive periods if more than two exist
    change_consecutive = {}
    for i in range(len(years) - 1):
        year1 = years[i]
        year2 = years[i+1]
        img1 = analysis_results[year1]['image']
        img2 = analysis_results[year2]['image']

        # Resample consecutive images to the target projection
        img1_res = img1.reproject(crs=target_proj.crs(), scale=target_scale)
        img2_res = img2.reproject(crs=target_proj.crs(), scale=target_scale)

        print(f"\nCalculating Change: {year1} vs {year2}...")
        change_stats = compute_change_stats(img1_res, img2_res, studyArea, target_scale)
        change_consecutive[f'{year1}_{year2}'] = change_stats
        print(change_stats)

    # --- C. Generate Comparison Graphs ---
    print("\nGenerating comparison graphs...")

    # Plot 1: Total Forest Area Over Time
    processed_years = list(analysis_results.keys())
    processed_years.sort() # Ensure years are in chronological order
    forest_areas = [analysis_results[year]['filtered_forest_ha'] for year in processed_years]

    plt.figure(figsize=(10, 6))
    plt.bar(processed_years, forest_areas, color=['#2ca02c', '#d62728', '#1f77b4'][:len(processed_years)]) # Use colors based on number of years
    plt.title('Total Filtered Forest Area (MMU Applied) Over Time', fontsize=16)
    plt.ylabel('Area (Hectares)', fontsize=12)
    plt.xlabel('Year', fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # Add text labels
    for i, v in enumerate(forest_areas):
        plt.text(i, v + 0.5, f"{v:.2f} ha", ha='center', va='bottom', fontsize=11)
    plt.show()

    # Plot 2: Detailed Change Dynamics for each period
    num_change_plots = len(years) - 1

    if num_change_plots > 0:
        fig, axes = plt.subplots(1, num_change_plots, figsize=(6 * num_change_plots, 7), sharey=True)
        fig.suptitle('Forest Change Dynamics (Hectares)', fontsize=20, y=1.05)

        # Ensure axes is an array even for a single plot
        if num_change_plots == 1:
            axes = [axes]

        for i in range(num_change_plots):
            year1 = years[i]
            year2 = years[i+1]
            plot_title = f'Change: {year1} vs {year2}'
            plot_change_barchart(change_consecutive[f'{year1}_{year2}'], plot_title, axes[i])

        plt.tight_layout()
        plt.show()

else: # Part of the if len(years) >= 2 logic
    print("\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    print(f" ERROR: Change analysis requires at least 2 successful periods, but found {len(years)}.")
    print(" Please ensure the main script ran successfully for at least two time periods.")
    print(" Periods found:", years)
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")

"""üìù Cell 8: Section 9 - Generate & Save Report
This cell defines functions to organize all the results (from client_results_all_years and change_consecutive) into a clean, table-based Markdown (.md) report. It then saves this report to your Google Drive in the GEE_Forest_Export folder.

Note: You must have mounted your Google Drive in Colab for this to work (usually by clicking the "Mount Drive" button in the file pane on the left).
"""

# ==========================================================================================
# Section 9: Generate and Export Report
# ==========================================================================================

print(f"\n--- Starting Report Generation ---")

# Helper function to organize results for the report
def organize_results_for_report(analysis_results, client_results_dict, change_results, CONFIG, sensorConfigs):
    """Organizes analysis and change detection results into dictionaries for the report."""
    organized_analysis = {}
    for year, results in client_results_dict.items():
        organized_analysis[year] = {
            'sensor': sensorConfigs[year]['name'],
            'scale': sensorConfigs[year]['scale'],
            'geometricalAreaHa': results['geometricalAreaHa'],
            'totalClassifiedAreaHa': results['totalClassifiedAreaHa'],
            'forestArea': results['forestArea'],
            'nonForestArea': results['nonForestArea'],
            'invalidPixelArea': results['invalidPixelArea'],
            'smoothedForestHa': results['smoothedForestHa'],
            'filteredForestHa': results['filteredForestHa'],
            'overallAccuracy': results['overallAccuracy'],
            'producersAccuracy': results['producersAccuracy'],
            'consumersAccuracy': results['consumersAccuracy'],
            'errorMatrix': results['errorMatrix'],
            'imageCount': results['imageCount'],
            'trainingSetSize': results['trainingSetSize'],
            'testingSetSize': results['testingSetSize']
        }
    organized_change = change_results # Directly use the calculated change results
    return organized_analysis, organized_change

# Helper function to generate a markdown report
def generate_markdown_report(analysis_results, change_results):
    """Generates a markdown report summarizing the analysis and change detection in tables."""
    report_content = "# Forest Change Detection Report\n\n"
    report_content += "This report summarizes the forest change detection analysis performed using Google Earth Engine.\n\n"
    report_content += "## 1. Analysis Results per Time Period\n\n"

    # Sort years for the report
    sorted_years = sorted(analysis_results.keys())

    # Create a table for analysis results
    analysis_headers = ["Year", "Sensor", "Scale (m)", "Geometrical Area (ha)", "Total Classified Area (ha)",
                        "Raw Forest Area (ha)", "Raw Non-Forest Area (ha)", "Invalid/NoData Area (ha)",
                        "Smoothed Forest Area (ha)", "Filtered Forest Area (ha)", "Overall Accuracy",
                        "Producer's Accuracy (Forest/Non-Forest)", "User's Accuracy (Forest/Non-Forest)",
                        "Image Count", "Training Samples", "Testing Samples"]
    report_content += "| " + " | ".join(analysis_headers) + " |\n"
    report_content += "| " + " | ".join(["---"] * len(analysis_headers)) + " |\n"

    for year in sorted_years:
        results = analysis_results[year]
        row_data = [
            year,
            results['sensor'],
            results['scale'],
            f"{results['geometricalAreaHa']:.2f}",
            f"{results['totalClassifiedAreaHa']:.2f}",
            f"{results['forestArea']:.2f}",
            f"{results['nonForestArea']:.2f}",
            f"{results['invalidPixelArea']:.2f}",
            f"{results['smoothedForestHa']:.2f}",
            f"{results['filteredForestHa']:.2f}",
            f"{results['overallAccuracy']:.4f}",
            results['producersAccuracy'],
            results['consumersAccuracy'],
            results['imageCount'],
            results['trainingSetSize'],
            results['testingSetSize']
        ]
        report_content += "| " + " | ".join(map(str, row_data)) + " |\n"

    report_content += "\n**Confusion Matrices (Actual Rows vs Predicted Columns):**\n\n"
    for year in sorted_years:
        report_content += f"**{year} Confusion Matrix:**\n"
        matrix = analysis_results[year]['errorMatrix']
        # Format confusion matrix as a table
        matrix_headers = ["Actual \\ Predicted", "Non-Forest (0)", "Forest (1)"]
        report_content += "| " + " | ".join(matrix_headers) + " |\n"
        report_content += "| " + " | ".join(["---"] * len(matrix_headers)) + " |\n"
        report_content += f"| Non-Forest (0) | {matrix[0][0]} | {matrix[0][1]} |\n"
        report_content += f"| Forest (1) | {matrix[1][0]} | {matrix[1][1]} |\n"
        report_content += "\n"

    report_content += "## 2. Forest Change Analysis\n\n"
    report_content += "Change analysis was performed by comparing the filtered classification results between time periods.\n\n"

    # Create a table for change results
    change_headers = ["Change Period", "Stable Non-Forest (ha)", "Reforestation (ha)",
                      "Deforestation (ha)", "Stable Forest (ha)"]
    report_content += "| " + " | ".join(change_headers) + " |\n"
    report_content += "| " + " | ".join(["---"] * len(change_headers)) + " |\n"

    # Add consecutive change results
    for key, stats in change_results.items():
        row_data = [
            key.replace('_', ' vs '),
            f"{stats['Stable Non-Forest']:.2f}",
            f"{stats['Reforestation']:.2f}",
            f"{stats['Deforestation']:.2f}",
            f"{stats['Stable Forest']:.2f}"
        ]
        report_content += "| " + " | ".join(map(str, row_data)) + " |\n"

    # Define the export path within the specified Google Drive folder
    exportFolder = 'GEE_Forest_Export'
    report_filename = 'Forest_Change_Report.md'
    # Assuming Google Drive is mounted at /content/drive/MyDrive/
    report_path = os.path.join('/content/drive/MyDrive/', exportFolder, report_filename) # Use os.path.join for compatibility

    # Check if the directory exists, if not, create it
    report_dir = os.path.dirname(report_path)
    if not os.path.exists(report_dir):
        os.makedirs(report_dir)
        print(f"Created directory: {report_dir}")

    # Write the report to the file
    with open(report_path, 'w') as f:
        f.write(report_content)
    return report_path

print("\n====================================================")
print(" GENERATING MARKDOWN REPORT")
print("====================================================")

try:
    # Consolidate all change results into one dictionary
    all_change_results = {}
    if 'change_consecutive' in locals(): # Check if change_consecutive was created
        all_change_results = change_consecutive.copy()

    if len(years) >= 2:
        if 'change_t1_t2' in locals():
            all_change_results[f'{years[0]}_{years[-1]}'] = change_t1_t2 # Add overall change

        organized_analysis, organized_change = organize_results_for_report(
            analysis_results=analysis_results,
            client_results_dict=client_results_all_years,
            change_results=all_change_results, # Pass the consolidated change results
            CONFIG=CONFIG,
            sensorConfigs=sensor_configs_by_year
        )

        report_path = generate_markdown_report(
            analysis_results=organized_analysis,
            change_results=organized_change
        )

        print(f"\n{'='*60}")
        print(f"SUCCESS: Report saved to Google Drive at: {report_path}")
        print(f"{'='*60}")

    else:
        # This 'else' catches the case where len(years) < 2
        print("\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        print(f" ERROR: Change analysis and report generation require at least 2 successful periods, but found {len(years)}.")
        print(" Please ensure the main script ran successfully for at least two time periods.")
        print(" Periods found:", years)
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")

except Exception as e:
    print(f"Error generating report: {e}")

"""üó∫Ô∏è Cell 9: Final Map Display
This final cell runs a quick projection check on the last processed image (if any) and then displays the interactive geemap map m. This map will contain all the layers (RGB composites, classifications, training points) that were added during the main loop in Cell 6.
"""

# This part might fail if all periods fail, so we wrap it too.
try:
    # Get projection info from the last successfully processed image
    last_year = sorted(analysis_results.keys())[-1] if analysis_results else None

    if last_year:
        proj_info = analysis_results[last_year]['image'].projection().getInfo()
        print('Projection Debug:')
        print(f' Composite Band Projection (CRS/Transform): {proj_info}')
        print(' Reason: Ensures consistent CRS and scale across the image.')

        # The cleaned layer for the last year is already added in the loop in Cell 6
        # No need to add it again here unless we want to ensure it's on top.
        # last_year_cleaned_image = analysis_results[last_year]['image']
        # classVis = {'min': 0, 'max': 1, 'palette': ['yellow', 'green'], 'opacity': 0.7}
        # if m:
        #      m.addLayer(last_year_cleaned_image.clip(studyArea), classVis, f'Final Classification ({last_year})', True)

    else:
        print("Could not get final projection info as no periods were processed successfully.")
        # print("Could not add final classification layer to map.") # Removed as layer is added in loop

except Exception as e:
    print(f"An error occurred in the final map cell. Error: {e}")

# Center and display the map
if m:
    # Ensure the map object is using the correct base map and centered
    m.set_options('SATELLITE') # Set options again
    m.center_object(studyArea, 14) # Center again
    display(m)
else:
    print("Geemap map object not initialized.")

"""#AI generated Report"""

# --- 1. Configuration and Authentication ---

# Set the target path for the final Markdown report
REPORT_PATH = '/content/drive/MyDrive/GEE_Forest_Export/Forest_Change_Report_AI.md'
PRIMARY_DATA_PATH = '/content/drive/MyDrive/GEE_Forest_Export/Forest_Change_Report.md' # Path to the report generated by the previous cell

# Authenticate the Gemini client
try:
    # Attempt to retrieve API key from Colab Secrets (recommended practice)
    # Use the correct secret name as defined in Colab Secrets
    API_KEY = userdata.get("AIzaSyAepvs3d0ZzAgQDK44Bsfbe-vOzPRpyT9s")
except Exception:
    # Fallback to environment variable check
    # Note: Using environment variables for API keys is less secure in Colab
    API_KEY = os.environ.get("AIzaSyAepvs3d0ZzAgQDK44Bsfbe-vOzPRpyT9s")

if not API_KEY:
    raise ValueError("API Key not found. Please set 'GOOGLE_API_KEY' in Colab Secrets or as an environment variable.")

client = genai.Client(api_key=API_KEY)
# Using a model optimized for complex, detailed, and structured text generation
# Consider using 'gemini-1.5-flash' for potentially lower cost and faster response
# if the prompt length is a concern and 'pro' is not strictly necessary.
MODEL_NAME = 'gemini-2.5-pro'


# --- 2. Extract and Prepare Primary Data from the Generated Report File ---

print(f"Reading primary data from: {PRIMARY_DATA_PATH}")
try:
    with open(PRIMARY_DATA_PATH, 'r') as f:
        primary_report_content = f.read()
    print("Successfully read primary report content.")
except FileNotFoundError:
    print(f"Error: Primary data file not found at {PRIMARY_DATA_PATH}")
    primary_report_content = "Error: Could not read primary data file."
except Exception as e:
    print(f"An error occurred while reading the primary data file: {e}")
    primary_report_content = f"Error: Could not read primary data file due to {e}"


# --- 3. Prompt Engineering for Exhaustive Report Generation ---

system_prompt = dedent("""
You are an expert GIS Analyst and Environmental Scientist with specialized knowledge in carbon project methodologies, specifically Verra VM0047 Afforestation, Reforestation, and Revegetation (ARR).
Your task is to transform structured report data from a Google Earth Engine (GEE) forest change analysis into a rigorous, formal, and exhaustive professional report titled: "Land Eligibility Assessment Framework under Verra VM0047 Afforestation, Reforestation, and Revegetation (ARR) Methodology".

The input data will be provided as Markdown text, primarily containing tables summarizing forest classification and change detection results over multiple time periods.

Your report must synthesize this data, interpreting the results (stable forest, stable non-forest, deforestation, reforestation) directly in the context of VM0047 land eligibility requirements. You must discuss:
- How the baseline forest cover and historical deforestation/reforestation trends inform eligibility for ARR activities.
- The implications of 'Stable Non-Forest' areas for potential afforestation/reforestation activities.
- The significance of 'Deforestation' areas for potential reforestation activities and baseline setting.
- The relevance of 'Reforestation' areas to historical trends and baseline validation.
- The impact of the analysis methodology (sensor shifts, labeling approaches like Hansen vs. Hansen+DW, Minimum Mapping Unit effects) on the eligibility assessment.

Your response must follow the strict outline provided below, creating a highly detailed and analytical narrative focused on the VM0047 methodology.
Adhere strictly to third-person perspective, use proper Markdown formatting for tables and headings, and maintain a highly technical tone. The report must be as long and detailed as possible, providing a thorough land eligibility assessment based on the provided data.

Required Report Outline:
I. Executive Summary: Land Eligibility Findings under VM0047 ARR Methodology
II. Technical Foundations and Methodological Adaptations for Eligibility Assessment (Include Table 1)
III. Classification Performance and Accuracy Assessment (Include Tables 2 & 3)
IV. Temporal Analysis of Forest Cover Area and Eligibility Implications (Focusing on Filtering Bias) (Include Table 4)
V. Forest Change Dynamics and VM0047 Eligibility Categories (As indicated by the change periods in the provided tables) (Include Table 5)
VI. Conclusion and Recommendations for VM0047 Project Development
""")

# The user prompt will now instruct the model to use the provided report content
# and specifically request the interpretation within the VM0047 framework.
user_prompt = dedent(f"""
Please generate the comprehensive report titled "Land Eligibility Assessment Framework under Verra VM0047 Afforestation, Reforestation, and Revegetation (ARR) Methodology" based on the following structured report data. Ensure all metrics and derived implications are thoroughly explained and interpreted strictly within the context of VM0047 land eligibility rules and definitions. Elaborate on all aspects presented in the data and their direct relevance to an ARR project feasibility study.

Structured Report Data:
--- REPORT DATA START ---
{primary_report_content}
--- REPORT DATA END ---
""")

# --- 4. Call the Gemini API ---

print(f"Starting Gemini generation using {MODEL_NAME}...")

# Define old_stdout before the try block to avoid NameError in finally
old_stdout = sys.stdout

try:
    # Pass the read report content to the model via the user prompt
    response = client.models.generate_content(
        model=MODEL_NAME,
        contents=[system_prompt, user_prompt]
    )

    generated_report = response.text
    print("Gemini generation successful.")

    # --- 5. Save the Generated Report to Google Drive ---

    # Ensure the directory exists before saving
    drive_dir = os.path.dirname(REPORT_PATH)
    if not os.path.exists(drive_dir):
        os.makedirs(drive_dir)

    with open(REPORT_PATH, 'w') as f:
        f.write(generated_report)

    print(f"\nReport successfully generated and saved to: {REPORT_PATH}")

except Exception as e:
    print(f"\nAn error occurred during API call or file writing: {e}")
    # Print the partial output if available for debugging
    if 'response' in locals() and response.text:
         print("\n--- PARTIAL REPORT OUTPUT FOR INSPECTION ---")
         print(response.text[:500] + "...")

finally:
    # Restore stdout
    sys.stdout = old_stdout